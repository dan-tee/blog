Title: Watch a single neuron learn
Date: 2016-02-22 20:00
Category: Machine Learning
Tags: Machine Learning, Neural Nets, Keras

The best way to learn an algorithm is to watch it in action. This is why I created the simplest
possible neural network in [Keras](http://keras.io). It's just a single neuron. We will train it
on the simplest nonlinear example.

I learned way more on this than I expected. So I will share it with you. This post will be as
visual as possible. No back prop [^0] calculation. Promised.

The data that we are going to predict is generated by a bilinear function that is `0` in the
interval `[0, 1]` and has the value `2(x-1)` on the interval `[1, 2]`.

{% notebook watch-a-single-neuron-learn.ipynb cells[0:1] %}

This is how a general neuron looks. In our case we will only have one input, the bias and one
output.

![Artificial neuron]({filename}/images/neuron.png)

Below you can see how to create one neuron in Keras. The `Dense` object is the grey circle from the diagram
above and the `Activation` object is the square.

{% notebook watch-a-single-neuron-learn.ipynb cells[1:2] %}

We had to take two choices here. One is the initialization of the weights. We chose
them to be randomly drawn from a normal distribution (... mean 0 and standard deviation 0.05).
The second choice is the activation function. The chosen
[ReLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
function looks similar to our data. It's `0` for `x<=0` and `x` for `x>0`. It actually performs
well in many real world applications.

Training of our neural network is done using back propagation of error.

Now let's train our neuron on the data by calling the `fit` on our Model.

The collapsed code is storing intermediate values from the training for visualization.

{% notebook watch-a-single-neuron-learn.ipynb cells[2:5] %}

We can now visualize the training by plotting the neurons predictions for each training iteration.

{% video /videos/neuron.mp4 400 300 /images/neuron_start.png %}

No big surprise here. The training reduces the prediction error. Let's see how the training error evolved.

{% notebook watch-a-single-neuron-learn.ipynb cells[5:6] %}

This might come as a little surprise. While it seemed in the animation like our neuron's predictions where only getting
better with every iteration, we see some jitter in the error chart. So with some iterations the error actually gets
worse.



[^0]: Detailed back prop explanation.